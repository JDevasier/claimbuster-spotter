{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClaimBuster-SVM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeBT8QjX9iT9"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqe0sBOmX4Ap"
      },
      "source": [
        "Links to Google Colab containing other claim spotting models can be found below:\r\n",
        "\r\n",
        "\r\n",
        "*   [Transformer/BERT-based](https://colab.research.google.com/github/idirlab/claimspotter/blob/master/adv_transformer/adv_transformer-notebook.ipynb)\r\n",
        "*   [BiLSTM](https://colab.research.google.com/github/idirlab/claimspotter/blob/master/bidirectional_lstm/bilstm-notebook.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsTE4bSmUDa0"
      },
      "source": [
        "Load imports and define which parts of speech we will use as features in the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1XYtFmSUCjm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5f6356b-b0d1-44a8-84b3-6fbf115b4b6e"
      },
      "source": [
        "# Copyright (C) 2020 IDIR Lab - UT Arlington\n",
        "#\n",
        "#     This program is free software: you can redistribute it and/or modify\n",
        "#     it under the terms of the GNU General Public License v3 as published by\n",
        "#     the Free Software Foundation.\n",
        "#\n",
        "#     This program is distributed in the hope that it will be useful,\n",
        "#     but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n",
        "#     GNU General Public License for more details.\n",
        "#\n",
        "#     You should have received a copy of the GNU General Public License\n",
        "#     along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
        "#\n",
        "# Contact Information:\n",
        "#     See: https://idir.uta.edu/cli.html\n",
        "#\n",
        "#     Chengkai Li\n",
        "#     Box 19015\n",
        "#     Arlington, TX 76019\n",
        "#\n",
        "\n",
        "import csv\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import pickle\n",
        "import joblib\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn.metrics as mt\n",
        "import matplotlib.pyplot as plt\n",
        "from shutil import rmtree\n",
        "from copy import deepcopy\n",
        "from itertools import chain\n",
        "from itertools import product\n",
        "from nltk import pos_tag\n",
        "from nltk import word_tokenize\n",
        "from numpy import argmax, interp, random\n",
        "from sklearn.utils._testing import ignore_warnings\n",
        "from sklearn.exceptions import ConvergenceWarning, UndefinedMetricWarning\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neighbors import KernelDensity\n",
        "\n",
        "\n",
        "POS = [\n",
        "    'MD', 'VBN', 'PRP$', 'CD', 'NNS', 'RBR', 'LS', 'WP', 'JJR', 'RB', 'WP$', 'VBZ', '-LRB-', 'CC', 'JJ', \n",
        "    '$', ':', 'VBG', \"''\", ',', 'WDT', 'EX', 'PDT', 'RP', '``', 'NNPS', 'NNP', 'FW', 'VB', 'PRP', 'RBS', \n",
        "    'DT', 'WRB', 'NN', '.', '-NONE-', 'IN', 'TO', 'UH', 'VBD', 'POS', 'VBP', 'JJS', 'SYM', '(', ')'\n",
        "]\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDVC7HHG9qx4"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1lEqCV1UNB-"
      },
      "source": [
        "## Metrics Functions\n",
        "Define helper functions to compute metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKSdaq-nUXPV"
      },
      "source": [
        "def compute_kde(x):\n",
        "    # https://scikit-learn.org/stable/auto_examples/neighbors/plot_kde_1d.html\n",
        "    x = np.asarray(x).reshape(-1, 1)\n",
        "    X_plot = np.linspace(0, 1, len(x))[:, np.newaxis]\n",
        "    fig, ax = plt.subplots()\n",
        "    colors = ['darkorange']\n",
        "    kernels = ['gaussian']\n",
        "    lw = 15\n",
        "\n",
        "    for color, kernel in zip(colors, kernels):\n",
        "        kde = KernelDensity(kernel=kernel, bandwidth=0.10).fit(x)\n",
        "        log_dens = kde.score_samples(X_plot)\n",
        "        ax.plot(X_plot[:, 0], np.exp(log_dens), color=color, lw=lw, linestyle='-')\n",
        "\n",
        "    ax.legend().remove()\n",
        "    ax.plot(x[:, 0], -0.005 - 0.01 * random.random(x.shape[0]), '+k', markersize=lw)\n",
        "\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_ylim(-0.06, 1.75)\n",
        "    plt.show()\n",
        "\n",
        "def compute_precisions(labels, scores, cutoff=None):\n",
        "    # https://github.com/apepa/clef2019-factchecking-task1/blob/master/scorer/task1.py#L81\n",
        "    combined = sorted([(scores[i], labels[i]) for i in range(len(scores))], reverse=True)\n",
        "    cutoff = min(cutoff or math.inf, len(combined))\n",
        "    combined = combined[:cutoff]\n",
        "    precisions = [0.0] * cutoff\n",
        "\n",
        "    for i, x in enumerate(combined):\n",
        "        if x[1] == 1:\n",
        "            precisions[i] += 1.0\n",
        "\n",
        "    for i in range(1, cutoff): # accumulate\n",
        "        precisions[i] += precisions[i - 1]\n",
        "    for i in range(1, cutoff): # normalize\n",
        "        precisions[i] /= i+1\n",
        "\n",
        "    return precisions\n",
        "\n",
        "\n",
        "def compute_average_precision(labels, scores, cutoff=None):\n",
        "    # https://github.com/apepa/clef2019-factchecking-task1/blob/master/scorer/task1.py#L52\n",
        "    combined = sorted([(scores[i], labels[i]) for i in range(len(scores))], reverse=True)\n",
        "    cutoff = min(cutoff or math.inf, len(combined))\n",
        "    combined = combined[:cutoff]\n",
        "    labels = [x[1] for x in combined]\n",
        "    precisions = []\n",
        "    num_correct = 0\n",
        "    num_positive = sum(labels)\n",
        "\n",
        "    for i, x in enumerate(combined):\n",
        "        if x[1] == 1:\n",
        "            num_correct += 1\n",
        "            precisions.append(num_correct / (i + 1))\n",
        "    \n",
        "    if precisions:\n",
        "        avg_prec = sum(precisions) / num_positive\n",
        "    else:\n",
        "        avg_prec = 0.0\n",
        "\n",
        "    return avg_prec\n",
        "\n",
        "def compute_dcg_term(i, labels, ver=1):\n",
        "    # Difference between version 0 and 1: https://en.wikipedia.org/wiki/Discounted_cumulative_gain#Discounted_Cumulative_Gain\n",
        "    return labels[i - 1] / math.log2(i + 1) if ver == 0 else ((1 << labels[i - 1]) - 1) / math.log2(i + 1)\n",
        "\n",
        "\n",
        "def compute_ndcg(labels, scores, cutoff=None):\n",
        "    # Precondition: for each index i, scores[i] corresponds with labels[i]\n",
        "    ver = 0\n",
        "    combined = sorted([(scores[i], labels[i]) for i in range(len(scores))], reverse=True)\n",
        "    cutoff = min(cutoff or math.inf, len(combined))\n",
        "    combined = combined[:cutoff]\n",
        "    labels = [x[1] for x in combined]\n",
        "\n",
        "    dcg = sum([compute_dcg_term(i, labels, ver=ver) for i in range(1, len(labels) + 1, 1)])\n",
        "    ideal_labels = sorted(labels, reverse=True)\n",
        "    idcg = sum([compute_dcg_term(i, ideal_labels, ver=ver) for i in range(1, len(labels) + 1, 1)])\n",
        "\n",
        "    try:\n",
        "        return dcg / idcg\n",
        "    except ZeroDivisionError:\n",
        "        return dcg / 0.0000000000001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0yTGSjiUyma"
      },
      "source": [
        "## Feature Vector Functions\n",
        "Helper functions to generate different parts of feature vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkpG9VuiU2yI"
      },
      "source": [
        "def get_ngram(file_name, tf_idf_vect, data, n, store_vocab=False, set_index=None):\n",
        "    sentence_term_tfidf = tf_idf_vect.transform(data.text)\n",
        "    sentence_term_df = pd.DataFrame(sentence_term_tfidf.todense(), columns=np.sort(list(tf_idf_vect.vocabulary_.keys())))\n",
        "\n",
        "    if set_index is not None:\n",
        "        sentence_term_df = sentence_term_df.set_index(set_index)\n",
        "\n",
        "    if store_vocab:\n",
        "        pd.DataFrame(sentence_term_df.columns).to_csv(\"models/vocabulary_\" + file_name + \".txt\", encoding='utf-8', index=False)\n",
        "\n",
        "    sentence_term_df.columns = sentence_term_df.columns.map(lambda x: 'W_' + str(n) + '_' + x)\n",
        "\n",
        "    return sentence_term_df\n",
        "\n",
        "\n",
        "def get_pos(data, n):\n",
        "    POSn = {x: 0 for x in product(POS, repeat=n)}\n",
        "    data_pos = pd.DataFrame(columns=POSn.keys())\n",
        "    data_pos.columns = data_pos.columns.map(lambda x: 'P_' + str(n) + '_' + '_'.join(x))\n",
        "\n",
        "    for idx, r in data.iterrows():\n",
        "        POSn = {x: 0 for x in product(POS, repeat=n)}\n",
        "        token_pos = pos_tag(word_tokenize(r.text))\n",
        "        token_pos = [y for (x, y) in token_pos]\n",
        "        token_pos = zip(*[token_pos[i:] for i in range(n)])\n",
        "\n",
        "        for tp in token_pos:\n",
        "            POSn[tp] += 1\n",
        "\n",
        "        data_pos.loc[idx] = list(POSn.values())\n",
        "    return data_pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJGJZRWvU5Lk"
      },
      "source": [
        "## Training and Cross Validation Functions\n",
        "Functions to train the final model and perform k-fold cross validation on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDbBtPR1U_Bj"
      },
      "source": [
        "def train_models(file_name, data, clfs, features, labels):\n",
        "    models = []\n",
        "\n",
        "    for feature_regex, feature_types in features:\n",
        "        for clf, name in clfs:\n",
        "            # Prepare training data with appropriate feature set\n",
        "            train_filter_regex = data.filter(regex=feature_regex)\n",
        "            train_filter_regex = train_filter_regex.sort_index(axis=1)\n",
        "\n",
        "            # Train and save the model for later use\n",
        "            clf.fit(train_filter_regex, data[labels])\n",
        "            j = joblib.dump(clf, 'models/' + name + '_' + feature_types + '_' + file_name + '.pkl')\n",
        "\n",
        "            models += [(name, (feature_regex, feature_types), clf)]\n",
        "\n",
        "            print(\"[\" + file_name + \"] \" + \"Training \" + name + '_' + feature_types + \" complete.\\n\")\n",
        "    return models\n",
        "\n",
        "\n",
        "@ignore_warnings(category=UndefinedMetricWarning)\n",
        "@ignore_warnings(category=ConvergenceWarning)\n",
        "def evaluate(file_name, data, clfs, features, labels, num_folds, num_classes=2):\n",
        "    print(\"K-folds total samples: \", len(data))\n",
        "    kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=2)\n",
        "    fold_count = 1\n",
        "\n",
        "    # ROC plot reference: https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py\n",
        "    for feature_regex, _feature_types in features:\n",
        "        for clf, clf_name in clfs:\n",
        "            cm = np.zeros((num_classes, num_classes)) \n",
        "            avg_ndcg = 0\n",
        "            avg_balanced_accuracy = 0\n",
        "            avg_ap = 0\n",
        "            aggregated_labels = []\n",
        "            aggregated_predictions = []\n",
        "            tprs = []\n",
        "            aucs = []\n",
        "            mean_fpr = np.linspace(0, 1, 100)\n",
        "            fig, ax = plt.subplots()\n",
        "\n",
        "            for train_index, test_index in kf.split(data, data[labels]):\n",
        "                # Mask data for the given fold\n",
        "                train = data.iloc[train_index]\n",
        "                test = data.iloc[test_index]\n",
        "\n",
        "                # Learn vocabulary and idf for the training data\n",
        "                fold_tf_idf_vect = TfidfVectorizer(min_df=1, ngram_range=(n_gram, n_gram))\n",
        "                fold_tf_idf_vect.fit(train.text)\n",
        "\n",
        "                # Vectorize training data\n",
        "                train = pd.concat([train, get_ngram(file_name, fold_tf_idf_vect, train, 1, set_index=train_index)], axis=1)\n",
        "                train = pd.concat([train, get_pos(train, 1)], axis=1)\n",
        "\n",
        "                # Vectorize testing data\n",
        "                test = pd.concat([test, get_ngram(file_name, fold_tf_idf_vect, test, 1, set_index=test_index)], axis=1)\n",
        "                test = pd.concat([test, get_pos(test, 1)], axis=1)\n",
        "\n",
        "                print(\"Fold: {0}, Training Samples: {1}, Testing Samples: {2}\".format(fold_count, len(train), len(test)))\n",
        "\n",
        "                # Train the model for this fold\n",
        "                train_filter_regex = train.filter(regex=feature_regex)\n",
        "                train_filter_regex = train_filter_regex.sort_index(axis=1)\n",
        "                clf.fit(train_filter_regex, train[labels])\n",
        "\n",
        "                # Evaluate the model for this fold\n",
        "                test_filter_regex = test.filter(regex=feature_regex)\n",
        "                test_filter_regex = test_filter_regex.sort_index(axis=1)\n",
        "                predictions = clf.predict(test_filter_regex)\n",
        "\n",
        "                fold_labels_list = test[labels].tolist()\n",
        "\n",
        "                # Aggreate some ROC metrics\n",
        "                viz = mt.plot_roc_curve(clf, test_filter_regex, test[labels], name='ROC fold {}'.format(fold_count), alpha=0.3, lw=1, ax=ax)\n",
        "                interp_tpr = interp(mean_fpr, viz.fpr, viz.tpr)\n",
        "                interp_tpr[0] = 0.0\n",
        "                tprs.append(interp_tpr)\n",
        "                aucs.append(viz.roc_auc)\n",
        "\n",
        "                # Aggregate fold results to generate an aggregated report at the end\n",
        "                aggregated_labels += fold_labels_list\n",
        "                aggregated_predictions += predictions.tolist()\n",
        "\n",
        "                cfs_scores = [x[1] for x in clf._predict_proba_lr(test_filter_regex).tolist()]\n",
        "\n",
        "                ndcg = compute_ndcg(fold_labels_list, cfs_scores)\n",
        "                avg_ndcg += ndcg\n",
        "\n",
        "                balanced_accuracy = mt.balanced_accuracy_score(test[labels], predictions)\n",
        "                avg_balanced_accuracy += balanced_accuracy\n",
        "\n",
        "                average_precision = compute_average_precision(fold_labels_list, cfs_scores)\n",
        "                avg_ap += average_precision\n",
        "\n",
        "                print('########## Classification Report ##########')\n",
        "                print(mt.classification_report(test[labels], predictions, digits=4))\n",
        "                \n",
        "                print('############ Confusion Matrix #############')\n",
        "                print(mt.confusion_matrix(test[labels], predictions), \"\\n\")\n",
        "\n",
        "                print(\"nDCG Score: \", ndcg)\n",
        "                print(\"Average Precision: \", average_precision)\n",
        "                print(\"Balanced Accuracy: \", balanced_accuracy, \"\\n\")\n",
        "                print(\"# --------------------------------------------------------- #\\n\")\n",
        "\n",
        "                cm += mt.confusion_matrix(test[labels], predictions)\n",
        "                fold_count += 1\n",
        "\n",
        "            print('# Aggregated K-Fold Classification Report #')\n",
        "            print(mt.classification_report(aggregated_labels, aggregated_predictions, digits=4))\n",
        "\n",
        "            print('### Aggregated K-Fold Confusion Matrix ####')\n",
        "            print(cm, \"\\n\")\n",
        "\n",
        "            print(\"Average nDCG: \", avg_ndcg / num_folds)\n",
        "            print(\"Mean Average Precision: \", avg_ap / num_folds)\n",
        "            print(\"Average Balanced Accuracy: \", avg_balanced_accuracy / num_folds, \"\\n\")\n",
        "            print(\"# --------------------------------------------------------- #\\n\")\n",
        "\n",
        "            mean_tpr = np.mean(tprs, axis=0)\n",
        "            mean_tpr[-1] = 1.0\n",
        "            mean_auc = mt.auc(mean_fpr, mean_tpr)\n",
        "            std_auc = np.std(aucs)\n",
        "            ax.plot(mean_fpr, mean_tpr, color='b', label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc), lw=2, alpha=.8)\n",
        "\n",
        "            std_tpr = np.std(tprs, axis=0)\n",
        "            tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "            tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "            ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "            ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05], title=\"[{0}] {1} ROC\".format(file_name, clf_name))\n",
        "            ax.legend(loc=\"lower right\")\n",
        "            plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOxiyDpXVDlR"
      },
      "source": [
        "## Load Trained Model Function\n",
        "Define function to load the trained model and score sentences using it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xorAfF2-xC0"
      },
      "source": [
        "def getCFSScore(file_name=None, transcript=\"\", models=[(\"SVM\", (\"length|W_|P_\", \"W_P\"), None)]):\n",
        "    def get_pos_vector(text):\n",
        "        dict_pos_count = {k: 0 for k in POS}\n",
        "        pos_tags = pos_tag(word_tokenize(text))  # TextBlob(text).pos_tags#\n",
        "\n",
        "        for _, tag in pos_tags:\n",
        "            if tag in dict_pos_count.keys():\n",
        "                dict_pos_count[tag] += 1\n",
        "\n",
        "        return dict_pos_count\n",
        "\n",
        "    # Load in the vocabulary and pre-calculated idf\n",
        "    vocabulary = pd.read_csv(os.path.join('models', 'vocabulary_{0}.txt'.format(file_name)))\n",
        "    count_vect = CountVectorizer(vocabulary=vocabulary['0'])\n",
        "    idf_ = pickle.load(open(os.path.join('models', 'idf_{0}'.format(file_name)), 'rb'))\n",
        "\n",
        "    # Convert sentence to dataframe and back up original text before vectorization\n",
        "    sentences = [transcript]\n",
        "    sentences = pd.DataFrame(sentences)\n",
        "    sentences.columns = ['text']\n",
        "    sentences['original_text'] = sentences.text\n",
        "    sentences.text = sentences['original_text'].map(lambda s: s.lower())\n",
        "    sentences['sentence_id'] = sentences.index\n",
        "\n",
        "    # Vectorize the sentence\n",
        "    sentence_word = count_vect.fit_transform(sentences.text)\n",
        "    sentence_word = sentence_word.toarray() * idf_\n",
        "    sentence_word = normalize(sentence_word, axis=1, norm='l2')\n",
        "    sentence_word = pd.DataFrame(sentence_word, columns=vocabulary['0'])\n",
        "    sentence_word = pd.concat([sentences.sentence_id, sentence_word], axis=1)\n",
        "    sentence_word = sentence_word.set_index('sentence_id')\n",
        "    sentence_word.columns = sentence_word.columns.map(lambda x: 'W_1_' + str(x))\n",
        "\n",
        "    # Get the part of speech vector for the sentence\n",
        "    sentence_pos = {sentences.sentence_id[i]: get_pos_vector(sentences.text[i]) for i in sentences.index}\n",
        "    sentence_pos = pd.DataFrame(sentence_pos).T\n",
        "    sentence_pos.columns = sentence_pos.columns.map(lambda x: 'P_1_' + str(x))\n",
        "    sentence_pos.index.names = ['sentence_id']\n",
        "\n",
        "    # Append sentence length info as a feature\n",
        "    sentence_pos['length'] = [len(sentences.loc[id].text.split()) for id in sentences.index]\n",
        "\n",
        "    # Concatenate all the features into one dataframe\n",
        "    data = pd.concat([sentence_pos, sentence_word], axis=1)\n",
        "\n",
        "    # Load in the models and return class prediction probabilities for the input sentence\n",
        "    for name, (feature_regex, feature_types), clf in models:\n",
        "        clf = joblib.load('models/' + name + '_' + feature_types + '_' + file_name + '.pkl')\n",
        "        model_name = clf.__class__.__name__\n",
        "        data_regex = data.filter(regex=feature_regex)\n",
        "        data_regex = data_regex.sort_index(axis=1)\n",
        "\n",
        "        if str(model_name) == 'LinearSVC':\n",
        "            return clf._predict_proba_lr(data_regex)[0]\n",
        "        else:\n",
        "            return clf.predict_proba(data_regex)[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7GQUeMt9u-F"
      },
      "source": [
        "# Get Data and Set Training Options"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nL1w54JVIz0"
      },
      "source": [
        "Download the necessary data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NxMeZf33p42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe562a91-35a5-42a3-96a7-0b58cf2324a9"
      },
      "source": [
        "!wget -O claimbuster_dataset.json https://github.com/idirlab/claimspotter/raw/master/data/two_class/kfold_25ncs.json\n",
        "!wget -O clef_dataset.tsv https://github.com/idirlab/claimspotter/raw/master/data/clef19/CT19-T1-Training.csv\n",
        "!wget -O clef_test.tsv https://github.com/idirlab/claimspotter/raw/master/data/clef19/CT19-T1-Test.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-09 17:55:00--  https://github.com/idirlab/claimspotter/raw/master/data/two_class/kfold_25ncs.json\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/idirlab/claimspotter/master/data/two_class/kfold_25ncs.json [following]\n",
            "--2021-02-09 17:55:01--  https://raw.githubusercontent.com/idirlab/claimspotter/master/data/two_class/kfold_25ncs.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1600093 (1.5M) [text/plain]\n",
            "Saving to: ‘claimbuster_dataset.json’\n",
            "\n",
            "claimbuster_dataset 100%[===================>]   1.53M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2021-02-09 17:55:01 (24.2 MB/s) - ‘claimbuster_dataset.json’ saved [1600093/1600093]\n",
            "\n",
            "--2021-02-09 17:55:01--  https://github.com/idirlab/claimspotter/raw/master/data/clef19/CT19-T1-Training.csv\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/idirlab/claimspotter/master/data/clef19/CT19-T1-Training.csv [following]\n",
            "--2021-02-09 17:55:02--  https://raw.githubusercontent.com/idirlab/claimspotter/master/data/clef19/CT19-T1-Training.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1660409 (1.6M) [text/plain]\n",
            "Saving to: ‘clef_dataset.tsv’\n",
            "\n",
            "clef_dataset.tsv    100%[===================>]   1.58M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2021-02-09 17:55:02 (19.1 MB/s) - ‘clef_dataset.tsv’ saved [1660409/1660409]\n",
            "\n",
            "--2021-02-09 17:55:02--  https://github.com/idirlab/claimspotter/raw/master/data/clef19/CT19-T1-Test.csv\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/idirlab/claimspotter/master/data/clef19/CT19-T1-Test.csv [following]\n",
            "--2021-02-09 17:55:02--  https://raw.githubusercontent.com/idirlab/claimspotter/master/data/clef19/CT19-T1-Test.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 732879 (716K) [text/plain]\n",
            "Saving to: ‘clef_test.tsv’\n",
            "\n",
            "clef_test.tsv       100%[===================>] 715.70K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-02-09 17:55:02 (14.6 MB/s) - ‘clef_test.tsv’ saved [732879/732879]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7gEgcWnVMiu"
      },
      "source": [
        "The main code to train, perform k-fold cross validation, and post-training evaluation.\n",
        "\n",
        "There are 4 pre-defined training options defined in the `training_options` dictionary:\n",
        "\n",
        "1.   `train_claimbuster_no_test`: Train the model on the ClaimBuster dataset and only do k-fold cross validation. No post training evaluation on a separate test set is performed.\n",
        "2.   `train_claimbuster_test_clef`: Train the model on the ClaimBuster dataset, do k-fold cross validation, and perform post training evaluation on the CLEF-2019 test dataset.\n",
        "3.   `train_clef_no_test`: Train the model on the CLEF-2019 dataset and only do k-fold cross validation. No post training evaluation on a separate test set is performed.\n",
        "4.   `train_clef_test_clef`: Train the model on the CLEF-2019 dataset, do k-fold cross validation, and perform post training evaluation on the CLEF-2019 test dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zd2wtvJ0Ws3s"
      },
      "source": [
        "training_options = {\n",
        "    \"train_claimbuster_no_test\": (100000000, 1000, \"claimbuster_dataset\", \"json\", \"\", \"\\t\", \"utf-8\", \"utf-8\", True, False, 1, 4, True),\n",
        "    \"train_claimbuster_test_clef\": (100000000, 1000, \"claimbuster_dataset\", \"json\", \"./clef_test.tsv\", \"\\t\", \"utf-8\", \"utf-8\", True, True, 1, 4, True),\n",
        "    \"train_clef_no_test\": (100000000, 1000, \"clef_dataset\", \"tsv\", \"\", \"\\t\", \"utf-8\", \"utf-8\", True, False, 1, 4, True),\n",
        "    \"train_clef_test_clef\": (100000000, 1000, \"clef_dataset\", \"tsv\", \"./clef_test.tsv\", \"\\t\", \"utf-8\", \"utf-8\", True, True, 1, 4, True)\n",
        "}\n",
        "\n",
        "(max_num_iters, max_num_iters_kfold, train_file_name, train_file_ext, \n",
        "test_file_path, csv_delimiter, train_file_encoding, test_file_encoding, \n",
        "test_file_has_header, test_file_in_multiple_doc_order, n_gram, num_folds, \n",
        "train_new_model) = training_options[\"train_clef_test_clef\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJbjn2fZ-ApW"
      },
      "source": [
        "# Train Model and Perform K-Fold Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sgjak6VZWxUE"
      },
      "source": [
        "Train the model using the training option defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TkKbNaPW4ES"
      },
      "source": [
        "# Reset models dir\n",
        "rmtree(\"./models\", ignore_errors=True)\n",
        "os.makedirs(\"models\", exist_ok=False)\n",
        "\n",
        "if train_new_model:\n",
        "    # Load the data and shuffle it\n",
        "    if train_file_ext in [\"csv\", \"tsv\"]:\n",
        "        data = pd.read_csv(f\"./{train_file_name}.{train_file_ext}\", encoding=train_file_encoding, delimiter=\",\", quotechar='\"')\n",
        "    elif train_file_ext in [\"json\"]:\n",
        "        pd.read_json(f\"./{train_file_name}.{train_file_ext}\", encoding=train_file_encoding)\n",
        "    data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    # Create a global tf_idf vectorizer with entire vocabulary\n",
        "    final_tf_idf_vect = TfidfVectorizer(min_df=1, ngram_range=(n_gram, n_gram))\n",
        "    final_tf_idf_vect.fit(data.text)\n",
        "    pickle.dump(final_tf_idf_vect.idf_, open(\"models/idf_\" + train_file_name, \"wb\"))\n",
        "\n",
        "    # Add in sentence length info to dataframe\n",
        "    sen_len = pd.DataFrame()\n",
        "    sen_len['length'] = [len(data.loc[id].text.split()) for id in data.index]\n",
        "    data = pd.concat([data, sen_len], axis=1)\n",
        "\n",
        "    # Vectorize final data for training the final model and storing for loading the model post training\n",
        "    final_data = pd.concat([data, get_ngram(train_file_name, final_tf_idf_vect, data, n_gram, True)], axis=1)\n",
        "\n",
        "    # Get PoS feature set for final data\n",
        "    final_data = pd.concat([final_data, get_pos(final_data, 1)], axis=1)\n",
        "\n",
        "    # Store final data in pickle files\n",
        "    j = joblib.dump(final_data, './models/DF_' + train_file_name + '.pkl')\n",
        "    final_data = joblib.load('./models/DF_' + train_file_name + '.pkl')\n",
        "\n",
        "    # Train the classifiers\n",
        "    classifiers = {\n",
        "        \"final_model\": [(LinearSVC(max_iter=max_num_iters), \"SVM\")],\n",
        "        \"k_fold_models\": [(LinearSVC(max_iter=max_num_iters_kfold), \"SVM\")]\n",
        "        }\n",
        "    features = [('length|W_|P_', 'W_P')]\n",
        "    models = train_models(train_file_name, final_data, classifiers[\"final_model\"], features, 'label')\n",
        "\n",
        "    # Evaluate the classifiers\n",
        "    evaluate(train_file_name, data, classifiers[\"k_fold_models\"], features, 'label', num_folds, num_classes=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78EBeuFJ-I0w"
      },
      "source": [
        "# Post Training Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9wt8FAiW8lX"
      },
      "source": [
        "If post-training evaluation was specified, then perform the appropriate evaluations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOlVgkrA3uto",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "outputId": "f5fbf842-c55d-4dd6-b4e6-71b0f89557b1"
      },
      "source": [
        "###############################################################\n",
        "# Post training and k-fold evaluation on test set starts here #\n",
        "###############################################################\n",
        "all_ground_truth_labels = []\n",
        "all_predicted_labels = []\n",
        "cfs_scores = []\n",
        "\n",
        "multi_test_doc_ground_truth_labels = []\n",
        "multi_test_doc_predicted_labels = []\n",
        "multi_test_doc_cfs_scores = []\n",
        "\n",
        "p_at_k_thresholds = [10, 20, 50]\n",
        "\n",
        "if test_file_path:\n",
        "    prev_sent_id = -1\n",
        "\n",
        "    if \".tsv\" in test_file_path or \".csv\" in test_file_path:\n",
        "        with open(test_file_path, encoding=test_file_encoding) as test_data_sv:\n",
        "            test_data = csv.reader(test_data_sv, delimiter=csv_delimiter, quotechar='\"')\n",
        "\n",
        "            if test_file_has_header:\n",
        "                next(test_data)\n",
        "\n",
        "            for d in test_data:\n",
        "                if int(d[0].split(',')[0]) < prev_sent_id and test_file_in_multiple_doc_order:\n",
        "                    multi_test_doc_ground_truth_labels.append(deepcopy(all_ground_truth_labels))\n",
        "                    multi_test_doc_predicted_labels.append(deepcopy(all_predicted_labels))\n",
        "                    multi_test_doc_cfs_scores.append(deepcopy(cfs_scores))\n",
        "\n",
        "                    all_ground_truth_labels = []\n",
        "                    all_predicted_labels = []\n",
        "                    cfs_scores = []\n",
        "                    prev_sent_id = int(d[0].split(',')[0])\n",
        "                else:\n",
        "                    prev_sent_id = int(d[0].split(',')[0])\n",
        "                \n",
        "                # Get label probabilities from trained model\n",
        "                cfs_score = getCFSScore(train_file_name, d[0].split(',')[2])\n",
        "                sentence_label = int(d[0].split(',')[-2])\n",
        "\n",
        "                # Aggregate ground truth and predicted labels for each sentence\n",
        "                all_ground_truth_labels.append(sentence_label)\n",
        "                all_predicted_labels.append(argmax(cfs_score))\n",
        "\n",
        "                # Aggregate CFS scores given by model\n",
        "                cfs_scores.append(cfs_score[1])\n",
        "            \n",
        "            if len(multi_test_doc_ground_truth_labels) and len(all_ground_truth_labels):\n",
        "                multi_test_doc_ground_truth_labels.append(deepcopy(all_ground_truth_labels))\n",
        "                multi_test_doc_predicted_labels.append(deepcopy(all_predicted_labels))\n",
        "                multi_test_doc_cfs_scores.append(deepcopy(cfs_scores))\n",
        "    else:\n",
        "        with open(test_file_path, encoding=test_file_encoding) as test_data_json:\n",
        "            test_data = json.load(test_data_json)\n",
        "\n",
        "            for d in test_data:\n",
        "                # Get label probabilities from trained model\n",
        "                cfs_score = getCFSScore(train_file_name, d[\"text\"])\n",
        "                sentence_label = int(d[\"label\"])\n",
        "\n",
        "                # Aggregate ground truth and predicted labels for each sentence\n",
        "                all_ground_truth_labels.append(sentence_label)\n",
        "                all_predicted_labels.append(argmax(cfs_score))\n",
        "\n",
        "                # Aggregate CFS scores given by model\n",
        "                cfs_scores.append(cfs_score[1])\n",
        "\n",
        "    if len(multi_test_doc_ground_truth_labels) == 0:\n",
        "        classification_report = mt.classification_report(all_ground_truth_labels, all_predicted_labels, digits=4)\n",
        "        confusion_matrix = mt.confusion_matrix(all_ground_truth_labels, all_predicted_labels)\n",
        "        balanced_accuracy = mt.balanced_accuracy_score(all_ground_truth_labels, all_predicted_labels)\n",
        "        average_precision = compute_average_precision(all_ground_truth_labels, cfs_scores)\n",
        "        ndcg = compute_ndcg(all_ground_truth_labels, cfs_scores)\n",
        "        try:\n",
        "            final_p_at_k = [x[th - 1] for x, th in zip(compute_precisions(all_ground_truth_labels, cfs_scores), p_at_k_thresholds)]\n",
        "        except IndexError:\n",
        "            final_p_at_k = \"Not enough test samples to calculate precisions at defined thresholds.\"\n",
        "    else:\n",
        "        merged_gt_labels = list(chain.from_iterable(multi_test_doc_ground_truth_labels))\n",
        "        merged_pred_labels = list(chain.from_iterable(multi_test_doc_predicted_labels))\n",
        "        classification_report = mt.classification_report(merged_gt_labels, merged_pred_labels, digits=4)\n",
        "        confusion_matrix = mt.confusion_matrix(merged_gt_labels, merged_pred_labels)\n",
        "\n",
        "        balanced_accuracy = sum([mt.balanced_accuracy_score(x, y) for x, y in zip(multi_test_doc_ground_truth_labels, multi_test_doc_predicted_labels)]) / len(multi_test_doc_ground_truth_labels)\n",
        "        average_precision = sum([compute_average_precision(x, y) for x, y in zip(multi_test_doc_ground_truth_labels, multi_test_doc_cfs_scores)]) / len(multi_test_doc_ground_truth_labels)\n",
        "        ndcg = sum([compute_ndcg(x, y) for x, y in zip(multi_test_doc_ground_truth_labels, multi_test_doc_cfs_scores)]) / len(multi_test_doc_ground_truth_labels)\n",
        "\n",
        "        final_p_at_k = [0.0] * len(p_at_k_thresholds)\n",
        "        precisions_at_k_pruned = []\n",
        "        precisions_at_k = [compute_precisions(x, y) for x, y in zip(multi_test_doc_ground_truth_labels, multi_test_doc_cfs_scores)]\n",
        "        try:\n",
        "            for p in precisions_at_k:\n",
        "                p_th = []\n",
        "\n",
        "                for th in p_at_k_thresholds:\n",
        "                    p_th.append(p[th - 1])\n",
        "                \n",
        "                precisions_at_k_pruned.append(p_th)\n",
        "\n",
        "            for i in range(0, len(p_at_k_thresholds)):\n",
        "                for p in precisions_at_k_pruned:\n",
        "                    final_p_at_k[i] += p[i]\n",
        "\n",
        "            final_p_at_k = [x / len(multi_test_doc_ground_truth_labels) for x in final_p_at_k]\n",
        "        except IndexError:\n",
        "            final_p_at_k = \"Not enough test samples to calculate precisions at defined thresholds.\"\n",
        "\n",
        "\n",
        "    print(\"###### Test Set Classification Report #####\\n\", classification_report)\n",
        "    print(\"######## Test Set Confusion Matrix ########\\n\", confusion_matrix, \"\\n\")\n",
        "    print(\"nDCG Score: \", ndcg)\n",
        "    print(\"(Mean) Average Precision: \", average_precision)\n",
        "    print(\"Balanced Accuracy: \", balanced_accuracy, \"\\n\")\n",
        "    print(\"Average Precision @ k: \", p_at_k_thresholds, final_p_at_k)\n",
        "    compute_kde(cfs_scores)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "###### Test Set Classification Report #####\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9843    0.9466    0.9651      6944\n",
            "           1     0.0771    0.2279    0.1152       136\n",
            "\n",
            "    accuracy                         0.9328      7080\n",
            "   macro avg     0.5307    0.5873    0.5401      7080\n",
            "weighted avg     0.9669    0.9328    0.9487      7080\n",
            "\n",
            "######## Test Set Confusion Matrix ########\n",
            " [[6573  371]\n",
            " [ 105   31]] \n",
            "\n",
            "nDCG Score:  0.46620781581107523\n",
            "(Mean) Average Precision:  0.10339332107772116\n",
            "Balanced Accuracy:  0.5948764126465547 \n",
            "\n",
            "Average Precision @ k:  [10, 20, 50] [0.14285714285714288, 0.1285714285714286, 0.09999999999999999]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}